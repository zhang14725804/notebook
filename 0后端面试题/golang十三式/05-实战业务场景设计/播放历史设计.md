架构设计优先级（读多写少）

    1. 缓存和消息队列
    2. 存储
    3. 中间件

## 播放历史架构设计

    功能模块
    架构设计
    存储架构
    可用性设计

## 功能模块

【objectID + businessType】实现平台化（视频历史，文章历史，漫画历史）

    变更功能：添加、删除、清空
    读取功能：按照timeline返回top-N，点查获取进度信息
    其他：暂停/恢复记录（可以暂停/恢复记录播放历史功能），首次观看增加经验

高【tps】写入，高【qps】读取的业务服务。分析清楚系统的hot path（瓶颈发生的地方）。

## 架构设计

    （1）API gatway
    （2）BFF（专注平台业务数据组织）
    （3）history Service（去平台化，专注历史数据，负责数据读、写、删除）
    （4）Kafka，redis，HBase（TiDB）
    （5）history Job（消费上游kafka数据，配合上游service批量打包的数据持久化）


【BFF划分原则】：垂类，重要性等等

【Write Through和Write Back】。Write Back思路：先写缓存，再由缓存写入数据库。

【缓存】抗读写。【消息队列】削峰，解决缓存和存储或用户流量和存储之间的速差。

**历史服务最重要的设计，就是批量打包(pipeline)聚合数据。将高频、密集的写请求先入缓存(write-back)，批量消费减少对存储的直接压力**

redis挂了会丢数据。history Service重启也会丢数据，Kafka也可能丢失数据

为什么使用【Write Back思路】：部分播放历史数据丢失可以接受，丢失影响比较小。

### 评论系统和播放历史

【评论系统】：立刻写Kafka消息队列，异步入MySQL。【播放历史】：实时写redis，再把redis数据读出来回刷HBase

### 播放历史删除后怎么做大数据分析

离线大数据架构，ODS操作数据层，保存原始数据来源：


    （1）【客户端埋点】（用行为追踪）：expose（观看视频），click，pv（page view）
    （2）【服务端埋点】：指标，日志，链路追踪，精细化埋点
    （3）【存储数据挖取】


【统一数据采集传输】

### history-service（历史数据处理模块）

写的核心逻辑：【last-write win】，聚合打包 + 批量写，不会无限存储，会按量截断

实时写redis -> 内存维护用户数据 -> 定时定量写入到Kafka

历史页面会cache-miss或读HBase，其他的都只是读redis，不会cache-miss

### history-job

### history-BFF

【用户首次播放、观看等加经验奖励积分功能】

【关闭播放历史的功能】，如何减少标记位缓存请求，如何节约QPS

## 存储设计

### 数据库设计（HBase）

### 缓存设计

首次行为，【bitmap、roaring bitmap、bloom filter】

避免热点问题，key sharding

## 可用性设计(数据一致性设计)

风险：

    （1）history-service重启过程，预聚合的消息丢失
    （2）history-job读取redis构建数据，但redis丢失


### 聚合优化（😅😅）

高收敛比的设计：上游流量大，下游聚合后流量小

鉴权功能使用长连接 + service模块数据上移

### 广播

用户首次触发的行为，需要发送消息给下游系统进行触发其他奖励等。如何减少这类一天只用一次的标记位缓存请求？

把flag状态广播到前端，根据前端状态值判断




